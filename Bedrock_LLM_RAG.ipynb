{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d01f727",
   "metadata": {},
   "source": [
    "#### Step 1 : Import Required Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcaf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required Libs\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ad0f9",
   "metadata": {},
   "source": [
    "#### Step 2 : Set AWS Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de5e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Environment is setup successfully\n"
     ]
    }
   ],
   "source": [
    "## Set AWS Region\n",
    "try:\n",
    "    # Set AWS Region\n",
    "    region = \"us-east-1\"  # Note the correct region code \"us-east-1\"\n",
    "    \n",
    "    # The Bedrock runtime must be initialized with your AWS credentials/profile and region to authenticate you \n",
    "    # and direct requests to the right foundation models or agent.\n",
    "    bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    \n",
    "    print(\"Project Environment is setup successfully\")\n",
    "    \n",
    "except (BotoCoreError, ClientError) as error:\n",
    "    print(f\"An error occurred while setting up AWS clients: {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d78ad",
   "metadata": {},
   "source": [
    "#### Step-3 : Setup RAG Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3fe83",
   "metadata": {},
   "source": [
    "###### 1. Take user query\n",
    "###### 2. Retrieve relevant information from our knowledge base\n",
    "###### 3. Generate a response using a foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6323fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------------------------------------------------------------------------------------------------------------------------------- \n",
    "## 1] RAG Model - Amazon Titan text foundation model available via Amazon Bedrock.\n",
    "##  This model is a large language model (LLM) optimized for a wide range of advanced, general-purpose language\n",
    "##  tasks such as open-ended text generation, conversational chat, and also supports Retrieval Augmented Generation (RAG) workflows.\n",
    "##  ---\n",
    "## 2] knowledge_base_id identifies the knowledge base to query.This is a unique identifier (typically a string or an Amazon Resource Name, ARN)\n",
    "##  that specifies which knowledge base your application or model should access during information retrieval.\n",
    "##  In the context of Amazon Bedrock: A knowledge base is a structured repository of information—often created by ingesting private documents,\n",
    "##  databases, or other content. Amazon Bedrock uses these knowledge bases to support Retrieval-Augmented Generation (RAG) workflows.\n",
    "##  The knowledge_base_id is the unique ID assigned to a specific knowledge base within Amazon Bedrock. When issuing a query,\n",
    "##  this ID tells Bedrock which knowledge base to search for relevant information.\n",
    "##---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "##\n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "def generate_rag_response(query,knowledge_base_id, model_id = model_id):\n",
    "    \"\"\" Generate Response using RAG Approach\n",
    "        Args :\n",
    "         1] query (str) : The user's question\n",
    "         2] knowledge_base_id (str) : ID of the knowledge base to query\n",
    "    \"\"\"\n",
    "    try: \n",
    "        \"\"\" Step-1 : Retrieve information from the knowledge base\n",
    "        # Note : In a full implemenation, we would call the Bedrock Knowledge Base API here\n",
    "        # For this example, we will simulate the retrieval with a placeholder\n",
    "        # bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name = region)\n",
    "        # kb_response = bedrock_agent_runtime.retrieve(\n",
    "           knowledgeBaseId=knowledge_base_id,\n",
    "           retrivalQuery ={'text' : query},\n",
    "           numberOfResults = 3)\n",
    "       \"\"\"\n",
    "       ## For simplified example:\n",
    "        retrieved_info =\"\"\" Employee Handbook Section 3.2 : Pait Time Off\n",
    "         All Employees accrue PTO at a rate of 1.5 days per month (18 days per year).\n",
    "         PTO requests must be submitted at least two weeks in advance through the HR portal\n",
    "         Unused PTO can be carried over to the following year, up to a maximum of 5 days\n",
    "         For further queries on PTO, please contact hr@example.com\n",
    "\n",
    "         Remote work policy for full time employees only - Full time employees are eligible to work remotely \n",
    "         up to 2 days per week and max of 10 days per month under unavoidable circumstances only when manager \n",
    "         has approved additional remote work days. Under normal circumstances, full time employees are expected to work from office for 3 days a week.\n",
    "         For consitency, co-ordination and efficiency purpose, all employees are expected to work from\n",
    "         office from Monday to Wednesday.\n",
    "                                          \"\"\"\n",
    "        \n",
    "        # Step 2 - Construct the prompt for the model using retrieved information and query\n",
    "        prompt = f\"\"\"\n",
    "          You are an HR assistant for our company.Use only following information to answer the query.\n",
    "          If you do not know the answer based on this information then say No - don't make up an answer\n",
    "\n",
    "          RETRIEVED INFORMATION :\n",
    "          {retrieved_info}\n",
    "\n",
    "          USER QUESTION:\n",
    "          {query}\n",
    "            \"\"\"\n",
    "    \n",
    "        # Step 3 - Construct the payload\n",
    "        # Generate a response using the specified model\n",
    "        if 'claude' in model_id.lower():\n",
    "            payload = {\n",
    "               \"anthropic_version\": \"bedrock-2023-05-31\", # Specify the version of the Anthropic model to use\n",
    "               \"max_tokens\": 512, # How many words (max tokens) the AI can respond with (up to 512).\n",
    "               \"temperature\" : 0.5, #How “creative” or random the answers should be (temperature 0.5 means moderately creative).\n",
    "               \"messages\" :[\n",
    "                    {\n",
    "                         \"role\": \"user\",# The role of the user in the conversation\n",
    "                         \"content\": prompt # The actual text of the user message, which includes the retrieved information and the user's question\n",
    "                     \n",
    "                  }\n",
    "                  \n",
    "               ]\n",
    "            }\n",
    "        else:\n",
    "            # Generic format for other models.\n",
    "            payload = {\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": 256,\n",
    "                    \"temperature\": 0.9\n",
    "                }\n",
    "            }\n",
    "        # Step 4 - Call the model   \n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload), # Convert the payload to JSON format\n",
    "            contentType=\"application/json\", # Specify the content type as JSON\n",
    "            accept=\"application/json\" # Specify the response format as JSON\n",
    "        )\n",
    "        # Step 5 - Parse the response based on the model type.\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8')) # Read and decode the response body\n",
    "        ###       \n",
    "        if 'claude' in model_id.lower():\n",
    "            generated_text = response_body[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            generated_text = response_body[\"results\"][0][\"outputText\"]\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while generating the response:{str (e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b57b60",
   "metadata": {},
   "source": [
    "#### Step 4:  Test RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2258e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User Query:  What is the role of my manager ?\n",
      "\n",
      " Response from RAG Model:  The manager is expected to approve additional remote work days if unavoidable circumstances permit\n"
     ]
    }
   ],
   "source": [
    "# Define a sample knowledge base ID - This would be a real ID in a production scenario\n",
    "sample_kb_id = \"sample hr knowledge base id\"  # Replace with your actual knowledge base ID\n",
    "# Test query\n",
    "# test_query = \"How many PTO days do full-time employees get per year?\"\n",
    "#test_query = \"Is current remote work policy is applicable for full time employees only? What about part time employees?\"\n",
    "test_query = \" What is the role of my manager ?\"\n",
    "# test_query = \"Why all employees are expected to work from office from Monday to Wednesday?\"  # Uncomment to use this query\n",
    "# # Call the function to generate a response\n",
    "response = generate_rag_response(test_query, sample_kb_id)\n",
    "print(\"\\n User Query:\", test_query)\n",
    "print(\"\\n Response from RAG Model:\", response)\n",
    "# --- End  ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b442c3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab71349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question about time off process:\n",
      "\n",
      "The process for requesting time off is by submitting a request through the HR portal at least two weeks in advance.\n",
      "\n",
      "Question about remote work policy (not in our example knowledge base):\n",
      " Full-time employees are permitted to work remotely up to two days per week and a maximum of ten days per month under unavoidable circumstances only when approved by the manager.\n"
     ]
    }
   ],
   "source": [
    "# Test with a question that should be answerable from our knowledge base\n",
    "test_query_2 = \"What's the process for requesting time off?\"\n",
    "response_2 = generate_rag_response(test_query_2, sample_kb_id)\n",
    "\n",
    "# Test with a question that might not be in our knowledge base\n",
    "test_query_3 = \"What's the company policy on remote work?\"\n",
    "response_3 = generate_rag_response(test_query_3, sample_kb_id)\n",
    "\n",
    "print(\"\\nQuestion about time off process:\")\n",
    "print(response_2)\n",
    "\n",
    "print(\"\\nQuestion about remote work policy (not in our example knowledge base):\")\n",
    "print(response_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a25581d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca61b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket hr-knowledge-base-257269733378 already exists\n"
     ]
    }
   ],
   "source": [
    "# Set up the Knowledge Base Using Vector Database\n",
    "# Step 1 - Create S3 bucket. Create markdown file containing HR Policies and store in AWS s3\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Generate a unique bucket name using your account ID\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "bucket_name = f\"hr-knowledge-base-{account_id}\"\n",
    "\n",
    "# Check if the bucket already exists\n",
    "def bucket_exists(bucket_name):\n",
    "    response = s3.list_buckets()\n",
    "    buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "    return bucket_name in buckets\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "if not bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bucket: {str(e)}\")\n",
    "else:\n",
    "    print(f\"Bucket {bucket_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766051a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample HR policy markdown file\n",
    "hr_policy = \"\"\" Below is a comprehensive 3-page sample HR policy addressing annual leave, maternity and paternity benefits, sick leave, casual leave, festival leaves, leave carry-forward/cash payout, and a professional certification voucher. This is structured as a policy section of an employee handbook in accordance with recent best practices, clearly outlining purpose, scope, entitlements, approval processes, and conditions.[1][2][3]\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Leave and Time-Off Policy\n",
    "\n",
    "### Purpose and Scope\n",
    "\n",
    "This policy guides all regular full-time and part-time employees regarding eligibility, accrual, usage, and management of various types of leave. It aims to enable a healthy work-life balance, demonstrate organizational care, and meet compliance standards. All sections apply company-wide except where statutory or contractual provisions specify otherwise.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.1. Annual Leave (Vacation Leave)\n",
    "\n",
    "- **Entitlement:** Each employee is entitled to 24 days of paid annual leave per calendar year, accrued monthly.\n",
    "- **Leave Carry Forward:** Up to 10 days of unused annual leave may be carried forward to the next calendar year. Surplus leave in excess of 10 days will lapse, except as described below.\n",
    "- **Cash Payout:** At year-end, 50% of surplus leave days (beyond the 10 carry-forward cap) will be paid out as a one-time cash benefit, calculated at the employee’s basic daily wage rate, subject to statutory deductions.\n",
    "- **Approval:** Employees must submit annual leave requests at least 2 weeks in advance. Approvals are subject to departmental work requirements.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.2. Sick Leave\n",
    "\n",
    "- **Entitlement:** Employees are eligible for up to 12 days of paid sick leave per year, non-cumulative.\n",
    "- **Documentation:** Medical certificate is required for absences exceeding 2 consecutive days. Unused sick leave lapses at year-end and cannot be carried forward or cashed out.\n",
    "- **Notification:** Employees must notify their manager and HR as early as possible on the first day of illness.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.3. Casual Leave\n",
    "\n",
    "- **Entitlement:** 8 days of paid casual leave annually, for personal, urgent, or unforeseen matters.\n",
    "- **Usage:** No more than 2 consecutive days without managerial approval; unused casual leave lapses at year-end.\n",
    "- **Application:** At least one day’s prior notice required unless in emergencies.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.4. Festival and Public Holidays\n",
    "\n",
    "- **Entitlement:** All declared national and company-specified festival/public holidays are paid days off.\n",
    "- **Optional Holidays:** Employees may select up to 2 optional festival days annually from a company-approved list for personal observance, in addition to declared holidays.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.5. Maternity and Paternity Benefits\n",
    "\n",
    "- **Maternity Leave:** 26 weeks of paid leave for eligible employees, as per statutory guidelines. Up to 8 weeks may be availed prior to expected delivery date.\n",
    "- **Paternity Leave:** 15 days of paid paternity leave within 3 months of childbirth.\n",
    "- **Adoption:** Employees adopting a child under 12 months: 12 weeks (primary caregiver) or 7 days (secondary caregiver) paid leave.\n",
    "- **Application:** Written application must be submitted to HR at least 4 weeks in advance, unless emergency circumstances apply. Medical/adoption documentation is required.\n",
    "\n",
    "***\n",
    "\n",
    "### 1.6. Leave Without Pay (LWP)\n",
    "\n",
    "- **Eligibility:** Applied upon exhaustion of all paid leave entitlements, subject to managerial approval.\n",
    "- **Termination or Resignation:** Accrued but unused annual leave will be compensated as per the leave carry forward/cash out policy upon separation from service.\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Certification Voucher Policy\n",
    "\n",
    "### 2.1. Professional Development Voucher\n",
    "\n",
    "- **Purpose:** To support employee upskilling and industry certification, each permanent employee is eligible for a voucher of up to **USD $500** per calendar year to reimburse the successful completion of a recognized industry certification.\n",
    "- **Eligibility and Approval:** The certification must be relevant to the employee’s job role and/or agreed career development plan. Prior written approval of the immediate manager and HR is required before enrollment.\n",
    "- **Reimbursement:** Reimbursement is provided upon submission of completion certificate and original receipts. Employees must agree to serve the company for at least 12 months after completion, or the voucher may be clawed back on a prorated basis.\n",
    "- **Limit:** One voucher per person per calendar year, subject to budget availability.\n",
    "\n",
    "***\n",
    "\n",
    "## 3. General Provisions & Enforcement\n",
    "\n",
    "- **Compliance:** All employees are obliged to adhere to the procedures for applying and reporting for leave and certification reimbursement.\n",
    "- **Non-Compliance:** Falsification of leave claims, persistent absenteeism, or non-approved absences may lead to disciplinary action as per the Company Code of Conduct.\n",
    "- **Policy Review:** This policy is subject to annual review and may be modified per statutory changes or management decision. Employees will be notified of any changes.\n",
    "\n",
    "***\n",
    "\n",
    "**Definitions:**  \n",
    "- *Paid leave:* Leave during which employee receives full salary/wages.  \n",
    "- *Carry forward:* Transferring accrued but unused leave to the next year.  \n",
    "- *Cash payout:* Direct monetary compensation for unused leave.\n",
    "\n",
    "**References:**  \n",
    "- All leave entitlements and provisions in this policy comply with applicable local labor laws and supersede prior policies where inconsistent.[2][3][4]\n",
    "\n",
    "***\n",
    "\n",
    "*This policy is accessible on the company intranet and will be provided to all new employees during induction. HR remains the custodian for policy implementation and clarification.*\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ef8606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR policy file uploaded to S3 bucket hr-knowledge-base-257269733378 successfully.\n"
     ]
    }
   ],
   "source": [
    "## Save the HR policy content to a markdown file in S3\n",
    "with open('hr_policy.md', 'w') as file:\n",
    "    file.write(hr_policy)\n",
    "# Upload the markdown file to the S3 bucket\n",
    "try:    \n",
    "    s3.upload_file('hr_policy.md', bucket_name, 'hr_policy.md')\n",
    "    print(f\"HR policy file uploaded to S3 bucket {bucket_name} successfully.\")  \n",
    "except Exception as e:\n",
    "    print(f\"Error uploading HR policy file to S3: {str(e)}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce24a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a python function to query the knowledge base and generate a response using the RAG model\n",
    "# import json\n",
    "# import boto3\n",
    "# #\n",
    "# model_id = \"amazon.titan-text-express-v1\"\n",
    "# def query_hr_knowledge_base(user_query, knowledge_base_id, model_id = model_id):\n",
    "#     \"\"\" Query HR Knowledge Base and generate a response using RAG Model\n",
    "#         Args :\n",
    "#          1] user_query (str) : The user's question\n",
    "#          2] knowledge_base_id (str) : ID of the knowledge base to query\n",
    "#     \"\"\"\n",
    "#     #----\n",
    "#     # Remember, bedrock_runtime is for direct model inference, while bedrock_agent_runtime is for agent interactions,\n",
    "#     # multi-turn flows, and advanced orchestration within AWS Bedrock. Both clients are required when your\n",
    "#     # application needs to both interact directly with models  and leverage Bedrock’s agent capabilities,\n",
    "#     # as each exposes different methods and API endpoints.\n",
    "#     #----\n",
    "#     # Multi-turn flows enable an AI agent or system to have multiple rounds of conversation with a user,\n",
    "#     # where each exchange depends on the previous context. Rather than responding to isolated, single-turn queries, \n",
    "#     # the agent maintains conversational memory—letting it ask follow-up questions,  clarify details, \n",
    "#     # and adapt its actions based on user responses\n",
    "#     #----\n",
    "#     # Advanced orchestration refers to the coordination and management of multiple AI components—models, tools, agents, \n",
    "#     # external APIs, data pipelines—so they work together seamlessly as a unified system\n",
    "#     #----\n",
    "#     # Initialize the Bedrock runtime client.\n",
    "#     #----\n",
    "#     bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "#     bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=region) # Agent runtime \n",
    "#     try:\n",
    "#         retrieve_response = bedrock_agent_runtime.retrieve(\n",
    "#             knowledgeBaseId=knowledge_base_id,\n",
    "#             retrievalQuery={'text': user_query\n",
    "#                             },\n",
    "#             retrievalConfiguration={\n",
    "#                 'vectorSearchConfiguration': {\n",
    "#                     numberOfResults: 5 # Retrieve top most 5 relevant results\n",
    "#                     }\n",
    "#                 }\n",
    "            \n",
    "#         )\n",
    "\n",
    "#         # Extract the retrieved information from the response\n",
    "#         retrived_passages = []\n",
    "#         for result in retrieve_response.get('retrievedResults',[]):\n",
    "#             content = result.get('content', {}).get('text', '')\n",
    "#             source = \"Unknown source\"\n",
    "#             location = result.get('location', {})\n",
    "#             if location:\n",
    "#                 if 's3location' in location:\n",
    "#                     source = location.get('s3location', {}).get('uri', source)\n",
    "#                 elif 'type' in location:\n",
    "#                     source = location.get('type', source)\n",
    "\n",
    "#             score  = result.get('score', 0)\n",
    "#             # Append the retrieved passage to the list\n",
    "#             retrived_passages.append({\n",
    "#                 'content': content,\n",
    "#                 'source': source,\n",
    "#                 relevance_score: score\n",
    "#             })\n",
    "#         # Step 2 - Prepare context from retrieved passages for the model\n",
    "#         context = \"\\n\\n\".join([f\"Passage: {p['content']}\" for p in retrived_passages])\n",
    "#         # If no context was retrieved, use a default message to inform the user\n",
    "#         if not context:\n",
    "#             context = \"No relevant information found in the knowledge base.\"\n",
    "#         # Step 3 - Construct the prompt for the model using retrieved information and query\n",
    "#         prompt = f\"\"\"\n",
    "#           You are an HR assistant for our company.Use only following information to answer the query.\n",
    "#           If you do not know the answer based on this information then say \n",
    "#           'No I do not have definitive answer for this as of now' - don't make up an answer\n",
    "\n",
    "#           Context:\n",
    "#           {context}\n",
    "\n",
    "#           Questions: {user_query}\n",
    "\n",
    "#           Answer:            \n",
    "#         \"\"\"\n",
    "#         # Step 4 - Construct the payload - Remember, the payload structure may vary based on the model type\n",
    "#         if \"anthropic\" in model_id.lower():\n",
    "#             print(\"Inside Anthropic Model\")\n",
    "#             payload = {\n",
    "#                 \"anthropic_version\": \"bedrock-2023-05-31\",  # Specify the version of the Anthropic model to use\n",
    "#                 \"max_tokens\": 512,  # How many words (max tokens) the AI can respond with (up to 512).\n",
    "#                 \"temperature\": 0.5,  # How “creative” or random the answers should be (temperature 0.5 means moderately creative).\n",
    "#                 \"messages\": [\n",
    "#                     {\n",
    "#                         \"role\": \"user\",  # The role of the user in the conversation\n",
    "#                         \"content\": prompt  # The actual text of the user message, which includes the retrieved information and the user's question\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         elif \"amazon.titan\" in model_id.lower():\n",
    "#             print(\"Inside Amazon Titan Model\")\n",
    "#             payload = {\n",
    "#                 \"inputText\": prompt,\n",
    "#                 \"textGenerationConfig\": {\n",
    "#                     \"maxTokenCount\": 512,  # Maximum number of tokens in the response\n",
    "#                     \"temperature\": 0.6, # Controls the randomness of the output\n",
    "#                     \"topP\": 0.9  # Controls the diversity of the output by limiting the probability mass considered\n",
    "#                 }\n",
    "#             }\n",
    "#         elif \"meta.llama\" in model_id.lower():\n",
    "#             print(\"Inside Meta Llama Model\")\n",
    "#             payload = {\n",
    "#                 \"inputText\": prompt,\n",
    "#                 \"textGenerationConfig\": {\n",
    "#                     \"maxTokenCount\": 512,  # Maximum number of tokens in the response\n",
    "#                     \"temperature\": 0.7,  # Controls the randomness of the output\n",
    "#                     \"topP\": 0.9  # Controls the diversity of the output by limiting the probability mass considered\n",
    "#                 }\n",
    "#             }\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported model ID: {model_id}. Please use a supported model ID for RAG workflows.\")\n",
    "        \n",
    "#         # Step 5 - Call the model\n",
    "#         invoke_response = bedrock_runtime.invoke_model(\n",
    "#             modelId=model_id,\n",
    "#             body=json.dumps(payload),  # Convert the payload to JSON format\n",
    "#             contentType=\"application/json\",  # Specify the content type as JSON\n",
    "#             accept=\"application/json\"  # Specify the response format as JSON\n",
    "#         )\n",
    "\n",
    "#         # Step 6 - Parse the response based on the model type.\n",
    "#         response_body = json.loads(invoke_response['body'].read().decode('utf-8'))  # Read and decode the response body\n",
    "#         #\n",
    "#         if \"anthropic\" in model_id.lower():\n",
    "#             print(\"Model used here is :\", model_id)\n",
    "#             generated_answer = response_body.get('content',[{}])[0].get(\"text\")\n",
    "#         elif \"amazon.titan\" in model_id.lower():\n",
    "#             print(\"Model used here is :\", model_id)\n",
    "#             generated_answer = response_body.get('results', [{}])[0].get(\"outputText\",'')\n",
    "#         elif \"meta.llama\" in model_id.lower():\n",
    "#             print(\"Model used here is :\", model_id)\n",
    "#             generated_answer = response_body.get('generation', '')\n",
    "#         else:\n",
    "#             generated_answer = \"Error : Could not parse response from the model. Please check the model ID and payload format.\"\n",
    "\n",
    "#         # Return the generated answer\n",
    "#         return {\n",
    "#             \"query\" : user_query,\n",
    "#             \"retrieved_passages\": retrived_passages,\n",
    "#             \"generated_answer\": generated_answer,   \n",
    "#             \"model_used\": model_id\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         {\n",
    "#             \"Error :\"   : str(e),\n",
    "#             \"query :\" : user_query,\n",
    "#             \"knowledge_base_id\" : knowledge_base_id,\n",
    "#             \"retrieved_passages\": [],\n",
    "#             \"model_id\": model_id\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1212d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "model_id = \"amazon.titan-text-express-v1\"  # Default\n",
    "region = \"us-east-1\"  # Change this to your AWS region\n",
    "\n",
    "def query_hr_knowledge_base(user_query, knowledge_base_id, model_id=model_id, region=region):\n",
    "    \"\"\"\n",
    "    Query HR Knowledge Base and generate a response using a RAG Model.\n",
    "    Args:\n",
    "        user_query (str): The user's question.\n",
    "        knowledge_base_id (str): ID of the knowledge base to query.\n",
    "        model_id (str): The model ID to use.\n",
    "        region (str): AWS region where Bedrock is deployed.\n",
    "    \"\"\"\n",
    "    bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "    bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=region)\n",
    "\n",
    "    try:\n",
    "        retrieve_response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=knowledge_base_id,\n",
    "            retrievalQuery={'text': user_query},\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': 5  # Top 5 relevant results\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract retrieved information\n",
    "        retrieved_passages = []\n",
    "        for result in retrieve_response.get('retrievedResults', []):\n",
    "            content = result.get('content', {}).get('text', '')\n",
    "            source = \"Unknown source\"\n",
    "            location = result.get('location', {})\n",
    "\n",
    "            if location:\n",
    "                if 's3location' in location:\n",
    "                    source = location.get('s3location', {}).get('uri', source)\n",
    "                elif 'type' in location:\n",
    "                    source = location.get('type', source)\n",
    "\n",
    "            score = result.get('score', 0)\n",
    "            retrieved_passages.append({\n",
    "                'content': content,\n",
    "                'source': source,\n",
    "                'relevance_score': score\n",
    "            })\n",
    "\n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([f\"Passage: {p['content']}\" for p in retrieved_passages])\n",
    "        if not context:\n",
    "            context = \"No relevant information found in the knowledge base.\"\n",
    "\n",
    "        # Construct prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an HR assistant for our company. Use only the following information to answer the query.\n",
    "        If you do not know the answer based on this information then say:\n",
    "        'No I do not have definitive answer for this as of now' — don't make up an answer.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {user_query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Select payload based on model type\n",
    "        if \"anthropic\" in model_id.lower():\n",
    "            print(\"Inside Anthropic Model\")\n",
    "            payload = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.5,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        elif \"amazon.titan\" in model_id.lower():\n",
    "            print(\"Inside Amazon Titan Model\")\n",
    "            payload = {\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": 512,\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            }\n",
    "        elif \"meta.llama\" in model_id.lower():\n",
    "            print(\"Inside Meta Llama Model\")\n",
    "            payload = {\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": 512,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {model_id}. Please use a supported model ID for RAG workflows.\")\n",
    "\n",
    "        # Call the model\n",
    "        invoke_response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(payload),\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        response_body = json.loads(invoke_response['body'].read().decode('utf-8'))\n",
    "\n",
    "        if \"anthropic\" in model_id.lower():\n",
    "            generated_answer = response_body.get('content', [{}])[0].get(\"text\", \"\")\n",
    "        elif \"amazon.titan\" in model_id.lower():\n",
    "            generated_answer = response_body.get('results', [{}])[0].get(\"outputText\", '')\n",
    "        elif \"meta.llama\" in model_id.lower():\n",
    "            generated_answer = response_body.get('generation', '')\n",
    "        else:\n",
    "            generated_answer = \"Error: Could not parse response from the model.\"\n",
    "\n",
    "        # Final return\n",
    "        return {\n",
    "            \"query\": user_query,\n",
    "            \"retrieved_passages\": retrieved_passages,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"model_used\": model_id\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"query\": user_query,\n",
    "            \"knowledge_base_id\": knowledge_base_id,\n",
    "            \"retrieved_passages\": [],\n",
    "            \"model_id\": model_id\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a268cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Id is : amazon.titan-text-express-v1\n",
      "Inside Amazon Titan Model\n",
      "\n",
      "============================================================\n",
      "Query: How many days I can work remotely in a month?\n",
      "\n",
      "No retrieved information found.\n",
      "\n",
      "Generated Answer:\n",
      " No I do not have definitive answer for this as of now\n"
     ]
    }
   ],
   "source": [
    "##Retrievel Example\n",
    "# Define a sample knowledge base ID - This would be a real ID in a production scenario\n",
    "#sample_kb_id = \"sample hr knowledge base id\"  # Replace with your actual knowledge base ID\n",
    "sample_kb_id = \"ZHJZPXTKEV\"\n",
    "print(f\"Model Id is : {model_id}\")\n",
    "\n",
    "# Test with a question that should be answerable from our knowledge base\n",
    "query = \"How many days I can work remotely in a month?\"\n",
    "# query = \"Who can avail Remote working benefit?\"\n",
    "result = query_hr_knowledge_base(query,sample_kb_id,model_id)\n",
    "if result is not None:\n",
    "    print(\"\\n\" + \"==\"*30)\n",
    "    print(f\"Query: {result['query']}\")\n",
    "if result['retrieved_passages']:\n",
    "    print(f\"\\nRetrieved Information:\\n{result['retrieved_passages'][0]['content']}\")\n",
    "else:\n",
    "    print(\"\\nNo retrieved information found.\")\n",
    "print(f\"\\nGenerated Answer:\\n {result['generated_answer']}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSBedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
